{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umln0h6MLQqD",
    "tags": []
   },
   "source": [
    "# Importando as bibliotecas\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rbsa2\\AppData\\Local\\Temp/ipykernel_38780/2828644452.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_built_with_cuda()\n",
    "tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2640625569948878519\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2252026676\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7828232581623758661\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "type(gpus)\n",
    "if gpus:\n",
    "  # Replicate your computation on multiple GPUs\n",
    "  c = []\n",
    "  for gpu in gpus:\n",
    "    with tf.device(gpu.name):\n",
    "        print(gpu.name)\n",
    "        pass\n",
    "gpu = gpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append('./')\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.layers import Convolution1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import ThresholdedReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import models\n",
    "import h5py\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import Input\n",
    "from tensorflow.keras import optimizers\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "import sklearn.metrics as sklm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    \"\"\"\n",
    "    Class to handle loading and processing of raw datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source,\n",
    "                 alphabet=\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\",\n",
    "                 input_size=1014, num_of_classes=3, alphabet_size=71):\n",
    "        \"\"\"\n",
    "        Initialization of a Data object.\n",
    "\n",
    "        Args:\n",
    "            data_source (str): Raw data file path\n",
    "            alphabet (str): Alphabet of characters to index\n",
    "            input_size (int): Size of input features\n",
    "            num_of_classes (int): Number of classes in data\n",
    "        \"\"\"\n",
    "        self.alphabet = alphabet\n",
    "        self.alphabet_size = len(self.alphabet)\n",
    "        self.dict = {}  # Maps each character to an integer\n",
    "        self.no_of_classes = num_of_classes\n",
    "        for idx, char in enumerate(self.alphabet):\n",
    "            self.dict[char] = idx + 1\n",
    "        self.length = input_size\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load raw data from the source file into data variable.\n",
    "\n",
    "        Returns: None\n",
    "\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for doc in self.data_source:\n",
    "            data.append(doc)\n",
    "        self.data = np.array(data)\n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Return all loaded data from data variable.\n",
    "\n",
    "        Returns:\n",
    "            (np.ndarray) Data transformed from raw to indexed form with associated one-hot label.\n",
    "\n",
    "        \"\"\"\n",
    "        data_size = len(self.data)\n",
    "        start_index = 0\n",
    "        end_index = data_size\n",
    "        batch_texts = self.data[start_index:end_index]\n",
    "        batch_indices = []\n",
    "        one_hot = np.eye(self.no_of_classes, dtype='int64')\n",
    "        classes = []\n",
    "        for c, s in batch_texts:\n",
    "            batch_indices.append(self.str_to_indexes(s))\n",
    "            c = int(c) - 1\n",
    "            classes.append(one_hot[c])\n",
    "        return np.asarray(batch_indices, dtype='int64'), np.asarray(classes)\n",
    "\n",
    "    def str_to_indexes(self, s):\n",
    "        \"\"\"\n",
    "        Convert a string to character indexes based on character dictionary.\n",
    "        \n",
    "        Args:\n",
    "            s (str): String to be converted to indexes\n",
    "\n",
    "        Returns:\n",
    "            str2idx (np.ndarray): Indexes of characters in s\n",
    "\n",
    "        \"\"\"\n",
    "        s = s.lower()\n",
    "        max_length = min(len(s), self.length)\n",
    "        str2idx = np.zeros(self.length, dtype='int64')\n",
    "        for i in range(1, max_length + 1):\n",
    "            c = s[-i]\n",
    "            if c in self.dict:\n",
    "                str2idx[i - 1] = self.dict[c]\n",
    "        return str2idx\n",
    "\n",
    "    def vectorize_sentences(self, data, char_indices):\n",
    "        X = []\n",
    "\n",
    "        for doc in data:\n",
    "        \n",
    "            x = [char_indices[w] if w in self.alphabet else 0 for w in doc.lower()]\n",
    "            x2 = np.eye(self.alphabet_size + 1)[x]\n",
    "            X.append(x2)\n",
    "            \n",
    "        return pad_sequences(X, maxlen=self.length, padding=\"pre\")\n",
    "\n",
    "    def get_target_data(self):\n",
    "        \"\"\"\n",
    "        Return all loaded data from data variable.\n",
    "\n",
    "        Returns:\n",
    "            (np.ndarray) Data transformed from raw to indexed form with associated one-hot label.\n",
    "\n",
    "        \"\"\"\n",
    "        data_size = len(self.data)\n",
    "        start_index = 0\n",
    "        end_index = data_size\n",
    "        batch_texts = self.data[start_index:end_index]\n",
    "        batch_indices = []\n",
    "        one_hot = np.eye(self.no_of_classes, dtype='int64')\n",
    "        classes = []\n",
    "        for c, s in batch_texts:\n",
    "            c = int(c)\n",
    "            classes.append(one_hot[c])\n",
    "        return np.asarray(classes)\n",
    "\n",
    "    def get_data_3dim(self):\n",
    "        '''\n",
    "        transform training input to shape Number_of_Rows X MaxLen X Alphabet_size\n",
    "        this is need to be done to avoid the use of embedding layer, wich is not supported on the Tflite converter\n",
    "        '''\n",
    "        char_indices = dict((c, i) for i, c in enumerate(self.alphabet,1))\n",
    "        indices_char = dict((i, c) for i, c in enumerate(self.alphabet,1))\n",
    "\n",
    "        \n",
    "        train_data = self.vectorize_sentences(self.data, char_indices)\n",
    "        train_data.shape\n",
    "        return train_data\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KSRqGGEH2_Ny"
   },
   "outputs": [],
   "source": [
    "CHARNN_DATA_CONFIG = \"D:/SiDi/Project/Modulo II/github/multimodal-sentiment-analysis/rbsa2/charnn_cnn/charcnn_data_config.json\"\n",
    "charcnn_data_config = pd.read_json(CHARNN_DATA_CONFIG)\n",
    "\n",
    "CNN_DATA_CONFIG = \"D:/SiDi/Project/Modulo II/github/multimodal-sentiment-analysis/rbsa2/charnn_cnn/nn_config.json\"\n",
    "nn_data_config = pd.read_json(CNN_DATA_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "94AqsbHfVdB1"
   },
   "outputs": [],
   "source": [
    "#data_config = json.load(open(\"charcnn_data_config.json\")) #Leitura do arquivo de configuracao\n",
    "data_config = charcnn_data_config #Leitura do arquivo de configuracao\n",
    "\n",
    "#nn_config = json.load(open(\"nn_config.json\")) # Leitura de arquivo de configuracao, quais sao os parametros\n",
    "nn_config = nn_data_config # Leitura de arquivo de configuracao, quais sao os parametros\n",
    "\n",
    "alphabet_size = data_config[\"data\"][\"alphabet_size\"]+1 #tamanho do alfabeto\n",
    "input_size = data_config[\"data\"][\"input_size\"] #Tamanho do input de zi\n",
    "\n",
    "fully_connected_layers = nn_config[\"char_cnn_zhang\"][\"fully_connected_layers\"] #Camadas Conectadas\n",
    "conv_layers = nn_config[\"char_cnn_zhang\"][\"conv_layers\"] #Camadas Convulacionasi\n",
    "threshold = nn_config[\"char_cnn_zhang\"][\"threshold\"] # Limiar\n",
    "dropout_p = nn_config[\"char_cnn_zhang\"][\"dropout_p\"] #Taxa de Dropout\n",
    "embedding_size = nn_config[\"char_cnn_zhang\"][\"embedding_size\"] # Tamnho do Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uFSgTy5kVdB2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH_URL  = \"D:/SiDi/Project/Modulo II/dataset/dataset_sidi_512.csv\"\n",
    "df = pd.read_csv(DATASET_PATH_URL, sep='\\t')\n",
    "\n",
    "print(len(df))\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_dfcmoTpVdB3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sent_text\n",
       "NEG    0.333333\n",
       "NEU    0.333333\n",
       "POS    0.333333\n",
       "Name: index, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica o balanceamento das classes\n",
    "classes = df.groupby('sent_text')['index'].nunique()/df.shape[0] #Agrupe por index\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eRrHD1w4VdB4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Funcao que remove quebra de linha\n",
    "def remove_spaces(cell):  \n",
    "    return cell.replace('\\n',' ')\n",
    "\n",
    "#Mapeaa os tipos de classes existente nos rotulos de predicoes\n",
    "def to_one_hot(y, class_mapping, num_classes):\n",
    "    y_int = y.map(class_mapping)\n",
    "    return to_categorical(y_int, num_classes)\n",
    "\n",
    "#X_text = df['text'].apply(remove_spaces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Numero de classes do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fdL7BAx0VdB5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEG': 0, 'NEU': 1, 'POS': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_classes = df['sent_text'].nunique()\n",
    "#Faz o mapeeamento da classe com o id\n",
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(df['sent_text']))}\n",
    "class_mapping\n",
    "#y_int = df['sent_text'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agrupamento e Separação por Rotulo da Classe (Sentimento da Texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICING_SIZE = 1000\n",
    "neg_df = df.groupby(['sent_text']).get_group(\"NEG\").sample(n=SLICING_SIZE, ignore_index=True)\n",
    "neu_df = df.groupby(['sent_text']).get_group(\"NEU\").sample(n=SLICING_SIZE, ignore_index=True)\n",
    "\n",
    "pos_df = df.groupby(['sent_text']).get_group(\"POS\").sample(n=SLICING_SIZE, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Text Datraframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_join_text = []\n",
    "y_join = []\n",
    "\n",
    "text_path = []\n",
    "y_total = []\n",
    "\n",
    "timesteps = 3\n",
    "y_train_lf = []\n",
    "X_lf_pp_train = []\n",
    "y_true_train_docs = []\n",
    "\n",
    "\n",
    "for i in range(0,SLICING_SIZE, 1):\n",
    "    # Negative Class\n",
    "    X_text = neg_df.iloc[i].to_frame().T['text']\n",
    "    y = to_one_hot(neg_df.iloc[i].to_frame().T['sent_text'], class_mapping, num_classes)\n",
    "    X_join_text.extend(X_text)\n",
    "    y_join.extend(y)\n",
    "\n",
    "    # Positive Class\n",
    "    X_text = neu_df.iloc[i].to_frame().T['text']\n",
    "    y = to_one_hot(neu_df.iloc[i].to_frame().T['sent_text'], class_mapping, num_classes)\n",
    "    X_join_text.extend(X_text)\n",
    "    y_join.extend(y)\n",
    "    \n",
    "    # Neutro Class\n",
    "    X_text = pos_df.iloc[i].to_frame().T['text']\n",
    "    y = to_one_hot(pos_df.iloc[i].to_frame().T['sent_text'], class_mapping, num_classes)\n",
    "    X_join_text.extend(X_text)\n",
    "    y_join.extend(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]], dtype=float32),\n",
       " array([1000, 1000, 1000], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_join,  axis=0, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2492   30 2981 ...   71 2305 1559]\n",
      "2400 600\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/50997928/typeerror-only-integer-scalar-arrays-can-be-converted-to-a-scalar-index-with-1d#answer-50997969\n",
    "st =  StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1234)\n",
    "for train_index, test_index in st.split(X_join_text, y_join):\n",
    "    print(train_index)\n",
    "    X_train_text, X_test_text = pd.Series(X_join_text)[train_index.astype(int)],pd.Series(X_join_text)[test_index.astype(int)]\n",
    "    \n",
    "    y_train, y_test = np.array(y_join)[train_index.astype(int)], np.array(y_join)[test_index.astype(int)]\n",
    "    print(len(train_index), len(test_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforma Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = Data(data_source=X_train_text,\n",
    "                         alphabet=data_config[\"data\"][\"alphabet\"],\n",
    "                         input_size=data_config[\"data\"][\"input_size\"],\n",
    "                         num_of_classes=data_config[\"data\"][\"num_of_classes\"],\n",
    "                         alphabet_size=data_config[\"data\"][\"alphabet_size\"])\n",
    "training_data.load_data()\n",
    "\n",
    "validation_data = Data(data_source=X_test_text,\n",
    "                         alphabet=data_config[\"data\"][\"alphabet\"],\n",
    "                         input_size=data_config[\"data\"][\"input_size\"],\n",
    "                         num_of_classes=data_config[\"data\"][\"num_of_classes\"],\n",
    "                         alphabet_size=data_config[\"data\"][\"alphabet_size\"])\n",
    "validation_data.load_data()\n",
    "    \n",
    "\n",
    "X_train_text = training_data.get_data_3dim()\n",
    "X_test_text = validation_data.get_data_3dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]], dtype=float32),\n",
       " array([800, 800, 800], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, axis=0, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]], dtype=float32),\n",
       " array([200, 200, 200], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, axis=0, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssjgpvFe91Wg"
   },
   "source": [
    "# Conjunto de Dados de Validacao textValidationCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oW8DDL5zVdB9"
   },
   "outputs": [],
   "source": [
    "def predict(model, X_test_text, batch_size=32):\n",
    "    y_predict = np.round(model.predict(X_test_text))\n",
    "    return y_predict\n",
    "\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [np.argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "class TextValidationCheckpoint(Callback):\n",
    "    def __init__(self, filepath, X_test_text,y_test, metric = 'kappa'):\n",
    "        self.X_test_text = X_test_text\n",
    "        self.y_test = y_test\n",
    "        self.metric = metric\n",
    "        self.max_metric = float('-inf')\n",
    "        self.max_metrics = None\n",
    "        self.filepath = filepath\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        predicted_labels = predict(self.model, self.X_test_text)\n",
    "        true_labels = one_hot_decode(self.y_test)\n",
    "        predicted_labels = one_hot_decode(predicted_labels)\n",
    "        eval_metrics = {\n",
    "            'accuracy' : sklm.accuracy_score(true_labels, predicted_labels),\n",
    "            'f1_micro' : sklm.f1_score(true_labels, predicted_labels, average='micro'),\n",
    "            'f1_macro' : sklm.f1_score(true_labels, predicted_labels, average='macro'),\n",
    "            #'f1_binary' : sklm.f1_score(true_labels, predicted_labels, average='weighted', pos_label = 1),\n",
    "            'kappa' : sklm.cohen_kappa_score(true_labels, predicted_labels)\n",
    "        }\n",
    "        eval_metric = eval_metrics[self.metric]\n",
    "        self.history.append(eval_metrics)\n",
    "        \n",
    "        if epoch > -1 and eval_metric > self.max_metric:\n",
    "            print(\"\\n\" + self.metric + \" improvement: \" + str(eval_metric) + \" (before: \" + str(self.max_metric) + \"), saving to \" + self.filepath)\n",
    "            self.max_metric = eval_metric     # optimization target\n",
    "            self.max_metrics = eval_metrics   # all metrics\n",
    "            #self.model.save(self.filepath)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_input (InputLayer)     [(None, 2880, 102)]       0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 2880, 50)          5150      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 2874, 64)          22464     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 958, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 952, 64)           28736     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 317, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 315, 64)           12352     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 313, 64)           12352     \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 311, 64)           12352     \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 309, 64)           12352     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 103, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6592)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               843904    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " encoded_text (Dense)        (None, 256)               33024     \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 999,969\n",
      "Trainable params: 999,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbsa2\\multimodal-sentiment-analysis\\multimodal-sentiment-analysis\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text_input = Input(shape=(input_size, alphabet_size), name='text_input', dtype='float32')\n",
    "\n",
    "x = Convolution1D(activation='tanh',\n",
    "                     input_shape=(input_size, alphabet_size), filters=50, kernel_size=1)(text_input)\n",
    "\n",
    "for cl in conv_layers:\n",
    "    x = Convolution1D(cl[0], cl[1])(x)\n",
    "    #x = ThresholdedReLU(threshold)(x)\n",
    "    if cl[2] != -1:\n",
    "        x = MaxPooling1D(cl[2])(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "for fl in fully_connected_layers:\n",
    "    x = Dense(fl)(x)\n",
    "    #x = ThresholdedReLU(threshold)(x)\n",
    "    x = Dropout(dropout_p)(x)\n",
    "\n",
    "encoded_text = Dense(256, activation='tanh', name='encoded_text')(x)\n",
    "\n",
    "output = layers.Dense(num_classes, activation='softmax', name='output')(encoded_text)\n",
    "model_text = Model(text_input, output)\n",
    "model_text.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "                    metrics=['acc'])\n",
    "print(model_text.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26\n",
      "  6/150 [>.............................] - ETA: 11s - loss: 1.0974 - acc: 0.3646WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0232s vs `on_train_batch_end` time: 0.0476s). Check your callbacks.\n",
      "150/150 [==============================] - ETA: 0s - loss: 1.0966 - acc: 0.3688\n",
      "accuracy improvement: 0.3333333333333333 (before: -inf), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 22s 107ms/step - loss: 1.0966 - acc: 0.3688 - val_loss: 1.1303 - val_acc: 0.3450\n",
      "Epoch 2/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 1.0821 - acc: 0.3887 - val_loss: 1.0886 - val_acc: 0.3783\n",
      "Epoch 3/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 1.0319 - acc: 0.4700\n",
      "accuracy improvement: 0.4116666666666667 (before: 0.3333333333333333), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 1.0319 - acc: 0.4700 - val_loss: 1.0059 - val_acc: 0.5083\n",
      "Epoch 4/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.9923 - acc: 0.4917\n",
      "accuracy improvement: 0.4716666666666667 (before: 0.4116666666666667), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 97ms/step - loss: 0.9923 - acc: 0.4917 - val_loss: 1.0127 - val_acc: 0.4883\n",
      "Epoch 5/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.9509 - acc: 0.5296\n",
      "accuracy improvement: 0.475 (before: 0.4716666666666667), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.9509 - acc: 0.5296 - val_loss: 0.9917 - val_acc: 0.5050\n",
      "Epoch 6/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.9199 - acc: 0.5533\n",
      "accuracy improvement: 0.495 (before: 0.475), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.9199 - acc: 0.5533 - val_loss: 0.9938 - val_acc: 0.5117\n",
      "Epoch 7/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8618 - acc: 0.5992\n",
      "accuracy improvement: 0.5466666666666666 (before: 0.495), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.8618 - acc: 0.5992 - val_loss: 0.9352 - val_acc: 0.5850\n",
      "Epoch 8/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.7858 - acc: 0.6471 - val_loss: 0.9422 - val_acc: 0.5667\n",
      "Epoch 9/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.7124 - acc: 0.6833\n",
      "accuracy improvement: 0.5766666666666667 (before: 0.5466666666666666), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.7124 - acc: 0.6833 - val_loss: 0.9905 - val_acc: 0.5683\n",
      "Epoch 10/26\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.6512 - acc: 0.7233 - val_loss: 0.9887 - val_acc: 0.5900\n",
      "Epoch 11/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.5827 - acc: 0.7471\n",
      "accuracy improvement: 0.5916666666666667 (before: 0.5766666666666667), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.5827 - acc: 0.7471 - val_loss: 0.9839 - val_acc: 0.6017\n",
      "Epoch 12/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.5147 - acc: 0.7829 - val_loss: 0.9968 - val_acc: 0.5983\n",
      "Epoch 13/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.4528 - acc: 0.8062 - val_loss: 1.4195 - val_acc: 0.5383\n",
      "Epoch 14/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.3739 - acc: 0.8471\n",
      "accuracy improvement: 0.5983333333333334 (before: 0.5916666666666667), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.3739 - acc: 0.8471 - val_loss: 1.1682 - val_acc: 0.6000\n",
      "Epoch 15/26\n",
      "150/150 [==============================] - 15s 100ms/step - loss: 0.3429 - acc: 0.8646 - val_loss: 1.2226 - val_acc: 0.5850\n",
      "Epoch 16/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.2746 - acc: 0.9004 - val_loss: 1.7866 - val_acc: 0.5450\n",
      "Epoch 17/26\n",
      "150/150 [==============================] - 16s 106ms/step - loss: 0.2396 - acc: 0.9071 - val_loss: 1.4775 - val_acc: 0.5850\n",
      "Epoch 18/26\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.1945 - acc: 0.9342 - val_loss: 1.5519 - val_acc: 0.5933\n",
      "Epoch 19/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.1428 - acc: 0.9483\n",
      "accuracy improvement: 0.6 (before: 0.5983333333333334), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.1428 - acc: 0.9483 - val_loss: 1.8230 - val_acc: 0.6067\n",
      "Epoch 20/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.1177 - acc: 0.9583 - val_loss: 2.0442 - val_acc: 0.5867\n",
      "Epoch 21/26\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.1031 - acc: 0.9712 - val_loss: 2.0919 - val_acc: 0.5867\n",
      "Epoch 22/26\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.0961 - acc: 0.9721\n",
      "accuracy improvement: 0.6083333333333333 (before: 0.6), saving to txt_tweet_text_v1_test.h5\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.0961 - acc: 0.9721 - val_loss: 2.1812 - val_acc: 0.6067\n",
      "Epoch 23/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.0712 - acc: 0.9804 - val_loss: 3.7759 - val_acc: 0.5167\n",
      "Epoch 24/26\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 0.0695 - acc: 0.9796 - val_loss: 2.6364 - val_acc: 0.5883\n",
      "Epoch 25/26\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 0.0621 - acc: 0.9867 - val_loss: 2.6955 - val_acc: 0.5817\n",
      "Epoch 26/26\n",
      "150/150 [==============================] - 15s 97ms/step - loss: 0.0498 - acc: 0.9892 - val_loss: 2.7240 - val_acc: 0.6017\n"
     ]
    }
   ],
   "source": [
    "n_repeats = 1\n",
    "exp_history = []\n",
    "optimize_for = 'accuracy'\n",
    "model_file = 'txt_tweet_text_v1_test.h5'\n",
    "for i in range(n_repeats):\n",
    "    checkpoint = TextValidationCheckpoint(model_file, X_test_text, y_test, metric=optimize_for)\n",
    "    model_text.fit(X_train_text, y_train, epochs=26, batch_size=16,\n",
    "                     validation_data=(X_test_text, y_test)\n",
    "                     ,callbacks = [checkpoint])\n",
    "    exp_history.append(checkpoint.max_metrics)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4125\n",
      "0.6083333333333333\n",
      "0 0.4125 0.6083333333333333 0.6081345723789299 ness_exp1_single-page_repeat-00.hdf5\n"
     ]
    }
   ],
   "source": [
    "avg_result = sum([m['kappa'] for m in exp_history]) / n_repeats\n",
    "avg_acc = sum([m['accuracy'] for m in exp_history]) / n_repeats\n",
    "print(avg_result)\n",
    "print(avg_acc)\n",
    "for i, r in enumerate(exp_history):\n",
    "    model_file = \"ness_exp1_single-page_repeat-%02d.hdf5\" % (i,)\n",
    "    print(str(i) + ' ' + str(r['kappa']) + ' ' + str(r['accuracy']) + ' ' + str(r['f1_macro']) + ' ' + model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( checkpoint.history, open( \"bt4sa-text-tweet-sent-multi-cnn.pickle\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text.save('v0.0.1_bt4sa-text-tweet-sent.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.299215316772461\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "y_pred = model_text.predict(X_test_text)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classification Metric Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.66      0.62       200\n",
      "           1       0.62      0.52      0.56       200\n",
      "           2       0.60      0.64      0.62       200\n",
      "\n",
      "    accuracy                           0.60       600\n",
      "   macro avg       0.60      0.60      0.60       600\n",
      "weighted avg       0.60      0.60      0.60       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test = np.argmax(y_test, axis=1) # Convert one-hot to index\n",
    "\n",
    "pred_class = []\n",
    "for i in range(len(y_pred)):\n",
    "  pred = np.where(y_pred[i] == np.amax(y_pred[i]))\n",
    "  pred_class.append(pred[0][0])\n",
    "    \n",
    "print(classification_report(Y_test, pred_class))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "0-2-multi-input-nn-hmm-30-mar-2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "multimodal-sentiment-analysis",
   "language": "python",
   "name": "multimodal-sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
